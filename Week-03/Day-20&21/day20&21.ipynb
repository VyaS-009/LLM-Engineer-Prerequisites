{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 20 – Model Serialization\n",
    "### Turning a trained ML model into a deployable service\n",
    "\n",
    "In this notebook, we’ll:\n",
    "- Load our tuned RandomForest churn model\n",
    "- Serialize it using Joblib\n",
    "- Save preprocessing objects (like Scaler)\n",
    "- Create a minimal FastAPI-ready prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb351ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa25828",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d971c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df.dropna(inplace=True)\n",
    "df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Feature selection (numerical + categorical)\n",
    "numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "categorical_features = ['Contract', 'InternetService', 'OnlineSecurity', 'TechSupport', 'PaymentMethod']\n",
    "\n",
    "X_categorical = pd.get_dummies(df[categorical_features], drop_first=True)\n",
    "X = pd.concat([df[numerical_features], X_categorical], axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa144f",
   "metadata": {},
   "source": [
    "## 2. Train and Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1aaba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85      1291\n",
      "           1       0.60      0.50      0.55       467\n",
      "\n",
      "    accuracy                           0.78      1758\n",
      "   macro avg       0.71      0.69      0.70      1758\n",
      "weighted avg       0.77      0.78      0.77      1758\n",
      "\n",
      "✅ Model and scaler saved successfully!\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model and scaler\n",
    "joblib.dump(model, 'random_forest_tuned.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print('✅ Model and scaler saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e055f",
   "metadata": {},
   "source": [
    "## 3. Create a Minimal Prediction Function (FastAPI-ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d370cc",
   "metadata": {},
   "source": [
    "- If we want we can use the tuned model from day-18, where we tuned the hyper parameters.\n",
    "- I have used the tuned model from day-18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e853f63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (1 = Churn, 0 = Retained): 0\n"
     ]
    }
   ],
   "source": [
    "def predict_churn(sample_dict):\n",
    "    # Load model and scaler\n",
    "    model = joblib.load('../Day-18/random_forest_tuned.pkl')\n",
    "    scaler = joblib.load('../Day-18/scaler.pkl')\n",
    "\n",
    "    sample = pd.DataFrame([sample_dict])\n",
    "    X_cat = pd.get_dummies(sample[categorical_features], drop_first=True)\n",
    "    X_num = sample[numerical_features]\n",
    "    X_final = pd.concat([X_num, X_cat], axis=1)\n",
    "\n",
    "    X_final = X_final.reindex(columns=X.columns, fill_value=0)\n",
    "    X_scaled = scaler.transform(X_final)\n",
    "    pred = model.predict(X_scaled)[0]\n",
    "    return int(pred)\n",
    "\n",
    "sample_input = {\n",
    "    'tenure': 12,\n",
    "    'MonthlyCharges': 70,\n",
    "    'TotalCharges': 840,\n",
    "    'Contract': 'Month-to-month',\n",
    "    'InternetService': 'Fiber optic',\n",
    "    'OnlineSecurity': 'No',\n",
    "    'TechSupport': 'No',\n",
    "    'PaymentMethod': 'Electronic check'\n",
    "}\n",
    "\n",
    "print('Prediction (1 = Churn, 0 = Retained):', predict_churn(sample_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Output:**\n",
    "- `random_forest_tuned.pkl`\n",
    "- `scaler.pkl`\n",
    "- FastAPI-ready predict function\n",
    "\n",
    "**Deliverable:** `day20_model_serialization.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eb0480",
   "metadata": {},
   "source": [
    "# Day 21 – End-to-End ML Pipeline\n",
    "### From Raw Data → Prediction → Logging\n",
    "\n",
    "We’ll now combine all parts into a complete automated churn prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "713d5940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib, logging\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logging.basicConfig(filename='pipeline_logs.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86b93b",
   "metadata": {},
   "source": [
    "## 1. Load Serialized Model & Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a279772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded serialized model and scaler\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load('../Day-18/random_forest_tuned.pkl')\n",
    "scaler = joblib.load('../Day-18/scaler.pkl')\n",
    "print('✅ Loaded serialized model and scaler')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6505ba",
   "metadata": {},
   "source": [
    "## 2. Define Automated Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44beeb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(new_data):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    X_cat = pd.get_dummies(new_data[categorical_features], drop_first=True)\n",
    "    X_final = pd.concat([new_data[numerical_features], X_cat], axis=1)\n",
    "    X_final = X_final.reindex(columns=X.columns, fill_value=0)\n",
    "    X_scaled = scaler.transform(X_final)\n",
    "    predictions = model.predict(X_scaled)\n",
    "    new_data['PredictedChurn'] = predictions\n",
    "    logging.info(f\"Run successful at {datetime.now()} with {len(new_data)} rows.\")\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f70cd4",
   "metadata": {},
   "source": [
    "## 3. Run Pipeline on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89fc627b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Contract</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>PredictedChurn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>70</td>\n",
       "      <td>840</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tenure  MonthlyCharges  TotalCharges        Contract InternetService  \\\n",
       "0      12              70           840  Month-to-month     Fiber optic   \n",
       "\n",
       "  OnlineSecurity TechSupport     PaymentMethod  PredictedChurn  \n",
       "0             No          No  Electronic check               0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pd.DataFrame([sample_input])\n",
    "output_df = run_pipeline(sample_df)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5244cd41",
   "metadata": {},
   "source": [
    "## 4. Batch Predictions & Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a4ddd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv')\n",
    "df_test['TotalCharges'] = pd.to_numeric(df_test['TotalCharges'], errors='coerce')\n",
    "df_test.dropna(inplace=True)\n",
    "df_test = df_test.sample(50, random_state=42)\n",
    "\n",
    "predictions = run_pipeline(df_test)\n",
    "predictions.to_csv('predictions.csv', index=False)\n",
    "print('✅ Batch predictions saved to predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51089ae",
   "metadata": {},
   "source": [
    "✅ **Outputs:**\n",
    "- `pipeline_logs.log`\n",
    "- `predictions.csv`\n",
    "\n",
    "**Deliverable:** `day21_pipeline_automation.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
