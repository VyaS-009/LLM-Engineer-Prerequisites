{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Day 2 ‚Äî Matrix Multiplication, Dot Product & Embeddings\n",
    "### Foundation of AI Math ‚Äî How Vectors, Matrices & Embeddings Power Models\n",
    "---\n",
    "Today We'll understand *why* dot product, cosine similarity, and matrix multiplication are the **heart of AI**, and how every neural layer or embedding uses them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Dot Product ‚Äî How Two Things Align\n",
    "The dot product measures **how much two vectors go in the same direction**. It‚Äôs the simplest mathematical form of ‚Äúsimilarity.‚Äù\n",
    "\n",
    "### Formula\n",
    "$$A \\cdot B = \\sum_i (A_i \\times B_i)$$\n",
    "\n",
    "- If both vectors point the same way ‚Üí large positive value\n",
    "- If opposite ‚Üí negative\n",
    "- If unrelated ‚Üí near zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: 24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([5, 1, 2])\n",
    "B = np.array([4, 2, 1])\n",
    "\n",
    "dot = np.dot(A, B)\n",
    "print('Dot Product:', dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ The result `24` means the users' preferences are quite aligned.\n",
    "If this were user ratings, it means both users like similar movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Cosine Similarity ‚Äî Scale-Free Comparison\n",
    "Dot product depends on vector *length*. If one user rates everything high, their scores inflate.\n",
    "Cosine similarity fixes that by normalizing both vectors.\n",
    "\n",
    "$$\\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\|\\|B\\|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.956182887467515\n"
     ]
    }
   ],
   "source": [
    "cos_sim = dot / (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "print('Cosine Similarity:', cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `1` ‚Üí same direction (identical taste)\n",
    "- `0` ‚Üí no relation\n",
    "- `-1` ‚Üí opposite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Matrix Multiplication ‚Äî Many Dot Products at Once\n",
    "Each cell in a matrix product is one dot product. This lets us compare *all users or all items* together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Raw Similarity Matrix:\n",
      " [[34 20  8]\n",
      " [20 20 12]\n",
      " [ 8 12 18]]\n"
     ]
    }
   ],
   "source": [
    "ratings = np.array([\n",
    "    [5, 3, 0],\n",
    "    [4, 0, 2],\n",
    "    [1, 1, 4]\n",
    "])\n",
    "\n",
    "sim_matrix = np.dot(ratings, ratings.T)\n",
    "print('User Raw Similarity Matrix:\\n', sim_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Each off-diagonal element = similarity between two users.\n",
    "Diagonal = self-similarity (always max)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Normalized Similarity ‚Äî Cosine for All Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      " [[1.         0.76696499 0.32338083]\n",
      " [0.76696499 1.         0.63245553]\n",
      " [0.32338083 0.63245553 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_sim_matrix = cosine_similarity(ratings)\n",
    "print('Cosine Similarity Matrix:\\n', cosine_sim_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Values between 0‚Äì1 show normalized similarity. Easier to interpret across users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Neural Networks: Dot Product Everywhere\n",
    "Every neuron computes:\n",
    "$$output = input \\cdot weights + bias$$\n",
    "So, deep learning is really millions of dot products and matrix multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Layer Output:\n",
      " [[1.9 1.1]\n",
      " [3.5 3.5]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 2], [3, 4]])  # inputs\n",
    "W = np.array([[0.2, 0.8], [0.6, 0.4]])  # weights\n",
    "b = np.array([0.5, -0.5])  # biases\n",
    "\n",
    "Y = np.dot(X, W) + b\n",
    "print('Neural Layer Output:\\n', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Each row in `Y` represents neuron activations for one input.\n",
    "This is what happens in every dense layer in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Embeddings ‚Äî Vectors With Meaning\n",
    "Embeddings map words, users, or items into numeric space.\n",
    "Similar entities are close together (high cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog: 0.99\n",
      "cat: 0.99\n",
      "apple: 0.42\n"
     ]
    }
   ],
   "source": [
    "texts = ['dog', 'cat', 'apple']\n",
    "embeddings = np.array([\n",
    "    [0.8, 0.2, 0.1],\n",
    "    [0.7, 0.3, 0.2],\n",
    "    [0.1, 0.8, 0.9]\n",
    "])\n",
    "\n",
    "query = np.array([[0.75, 0.25, 0.15]])\n",
    "\n",
    "sims = cosine_similarity(query, embeddings)\n",
    "for t, s in zip(texts, sims[0]):\n",
    "    print(f'{t}: {s:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Similar words (like 'dog' and 'cat') have higher scores.\n",
    "This is how search and recommendation systems understand semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Reflection\n",
    "- What does dot product actually *mean* in similarity terms?\n",
    "    It actually finds how much similar both vectors are by checking at their direction.\n",
    "\n",
    "- Why does normalization (cosine) change the result?\n",
    "    It removes the bias towards large values, like if a user rates everything high (like 9 or 10) and another uses smaller numbers (like 1‚Äì5), it'll make the dot product large, even if the patterns differ.\n",
    "\n",
    "- Why is matrix multiplication the backbone of neural networks?\n",
    "    Every neuron in a neural network computes:\n",
    "        output= input.weights+bias\n",
    "    This is a dot product again.\n",
    "    Matrix multiplication lets a layer compute all neurons‚Äô activations for all samples at once\n",
    "\n",
    "- What does an embedding capture that raw numbers can‚Äôt?\n",
    "    An embedding captures the context and relationships between items by placing similar items close together in a conceptual \"space,\" something a simple, raw number can't do. This allows a computer to understand concepts like \"king is to queen as man is to woman\" just by doing math on their \"locations.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
